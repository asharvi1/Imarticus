data = read.csv('2008.csv')
getwd()
setwd("/Users/ravikiran/dektop/R assignment/")
setwd("/Users/ravikiran/dektop/R assignment/")
q()
q()
swirl()
library(swirl)
swirl()
head(flags)
dim(flags)
class(flags)
cls_list <- lapply(flags, class)
cls_list
class(cls_list)
as.character(cls_list)
cls_list <- sapply(flags, class)
cls_vect <- sapply(flags, class)
class(cls_vect)
sum(flags$orange)
flag_color <- flags[ , 11:17]
flag_colors <- flags[ , 11:17]
head(flag_colors)
lapply(flag_colors, sum)
sapply(flag_colors, sum)
sapply(flag_colors, mean)
flag_shapes <- flags[ ,19:23]
lapply(flag_shapes, range)
sapply(flag_shapes, range)
shape_mat <- sapply(flag_shapes, range)
shape_mat
class(shape_mat)
unique(c(3, 4, 5, 5, 5, 6, 6))
unique_vals <- lapply()
unique_vals <- lapply(flags, unique)
unique_vals
lapply(unique_vals, length)
sapply(unique_vals, length)
sapply(flags, unique)
lapply(unique_vals, function(elem) elem[2])
sapply(flags, unique)
vapply(flags, unique, numeric(1))
ok()
sapply(flags, class)
vapply(flags, class, character(1))
?tapply
table(flags$landmass)
table(flags$animate)
tapply(flags$animate, flags$landmass, mean)
tapply(flags$population, flags$red, summary)
table(flags$population)
tapply(flags$population, flags$landmass, summary)
library(datasets)
data("iris")
?iris
tapply(iris$Sepal.Length, iris$Species, summary)
tapply(iris$Sepal.Length, iris$Species, round(mean, 2))
tapply(iris$Sepal.Length, iris$Species, mean)
head(iris)
apply(iris[, 1:4], 2, mean)
apply(iris[, 1:4], 1, mean)
?apply
apply(iris[, 1:4], mean)
library(mtcars)
library(datasets)
data("mtcars")
mean(mtcars$mpg, mtcars$cyl)
with(mtcars, tapply(mpg, cyl, mean))
tapply(mtcars$mpg, mtcars$cyl, mean)
dim(mtcars)
colnames(mtcars)
apply(mtcars, 2, mean)
split(mtcars, mtcars$cyl)
tapply(mtcars$cyl, mtcars$mpg, mean)
sapply(split(mtcars$mpg, mtcars$cyl), mean)
sapply(mtcars, cyl, mean)
tapply(mtcars$hp, mtcars$cyl, mean)
sample <- tapply(mtcars$hp, mtcars$cyl, mean)
sample
sample(1)
sample(2)
sample
class(sample)
sample[1]
sample[3] - sample[1]
debug(ls)
ls
ls
?ls
swirl()
library(swirl)
swirl()
library(iris)
library(datasets)
data("iris")
?lapply
swirl()
head(flags)
dim(flags)
class(flags)
cls_list <- lapply(flags, class)
cls_list
class(cls_list)
as.character(cls_list)
cls_vect <- sapply(flags, class)
class(cls_vect)
sum(flags$orange)
flag_colors <- flags[,11:17]
head(flag_colors)
lapply(flag_colors, sum)
sapply(flag_colors, sum)
sapply(flag_colors,mean)
flag_shapes <- flags[,19:23]
lapply(flag_shapes, range)
shape_mat <- sapply(flag_shapes, range)
shape_mat
class(shape_mat)
unique(c(3, 4, 5, 5, 5, 6, 6))
unique_vals <- lapply(flags, unique)
unique_vals
lapply(unique_vals, length)
sapply(unique_vals, length)
sapply(flags, unique)
lapply(unique_vals, function(elem) elem[2])
sapply(flags, unique)
vapply(flags, unique, numeric(1))
ok()
sapply(flags, class)
vapply(flags, class, character(1))
?tapply
table(flags$landmass)
table(flag$animate)
table(flags$animate)
tapply(flags$animate, flags$landmass, mean)
tapply(flags$population, flags$red, summary)
tapply(flags$population, flags$landmass, summary)
library(datasets)
data(iris)
head(iris)
tapply(iris$Sepal.Length, iris$Species, mean)
apply(iris, 1, mean)
colMeans(iris)
apply(iris[, 1:4], 1, mean)
rowMeans(iris[, 1:4])
apply(iris, 2, mean)
apply(iris[, 1:4], 2, mean)
library(datasets)
data(mtcars)
sapply(mtcars, cyl, mean)
head(mtcars)
tapply(mtcars$cyl, mtcars$mpg, mean)
tapply(mtcars$mpg, mtcars$cyl, mean)
sapply(split(mtcars$mpg, mtcars$cyl), mean)
apply(mtcars, 2, mean)
with(mtcars, tapply(mpg, cyl, mean))
mean(mtcars$mpg, mtcars$cyl)
apply(mtcars, 2, mean)
split(mtcars, mtcars$cyl)
lapply(mtcars, mean)
sapply(mtcars, cyl, mean)
tapply(mtcars$hp, mtcars$cyl, mean)
209.21429 - 82.63636
ls
debug(ls)
ls
x <- c(rnorm(10), runif(10))
install.packages('RMySQL')
install.packages("arules")
library("arules", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
dataset = longley
linear_model = lm(formula = Employed ~ GNP, data = dataset)
# Visualising the residual values in the graph to identify the outliers
qqnorm(resid(linear_model))
plot(dataset$GNP, dataset$Employed)
plot(dataset$GNP, dataset$Employed, main = 'Simple Linear', xlab = 'GNP', ylab = 'Employed')
abline(lm(Employed ~ GNP))
abline(lm(Employed ~ GNP, data = dataset))
plot(dataset$GNP, dataset$Employed, main = 'Simple Linear', xlab = 'GNP', ylab = 'Employed', col = 'red')
abline(lm(Employed ~ GNP, data = dataset))
plot(dataset$GNP, dataset$Employed, main = 'Simple Linear', xlab = 'GNP', ylab = 'Employed', col = 'red', pch = 21)
abline(lm(Employed ~ GNP, data = dataset))
plot(dataset$GNP, dataset$Employed, main = 'Simple Linear', xlab = 'GNP', ylab = 'Employed', col = 'red', pch = 16)
abline(lm(Employed ~ GNP, data = dataset))
View(dataset)
dataset = dataset[, c(2,7)]
View(dataset)
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
ggplot(data = dataset, aes(x =  dataset$GNP, y = dataset$Employed)) +
geom_point(shape = 1) +
geom_smooth(method = lm)
ggplot(data = dataset, aes(x =  dataset$GNP, y = dataset$Employed)) +
geom_point(shape = 2) +
geom_smooth(method = lm)
ggplot(data = dataset, aes(x =  dataset$GNP, y = dataset$Employed)) +
geom_point(shape = 3) +
geom_smooth(method = lm)
ggplot(data = dataset, aes(x =  dataset$GNP, y = dataset$Employed)) +
geom_point(shape = 4) +
geom_smooth(method = lm)
ggplot(data = dataset, aes(x =  dataset$GNP, y = dataset$Employed)) +
geom_point() +
geom_smooth(method = lm)
load("/Users/ravikiran/Desktop/R/Air Quality UCI/airquality.RData")
a = c(2,1,2,3,5,6,7,8,2)
sum(a[a == 2])
dataset = mtcars
# Creating a regression model
regressor = lm(mpg~cyl+wt+am+gear+carb, data = dataset)
summary(regressor)$coefficients
summary(regressor)$estimate
summary(regressor)$Estimate
summary(regressor)coefficients
summary(regressor)$coefficients
library("QuantPsyc", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
lm.beta(regressor)
summary(regressor)$coefficients
setwd("~/Desktop/Imarticus/Academics/Assignments/Football Dataset/Linear Regression")
dataset = read.csv('FullData.csv')
# Removing the date of birth of a player, as it is not be a useful variable to calculate the rating of a player
dataset = dataset[-3]
# Splitting the data to training and test set with "caTools" package
library(caTools)
set.seed(123)
split = sample.split(dataset$Rating, SplitRatio = 0.7)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
regressor_lm_1 = lm(formula = Rating ~ ., data = training_set)
summary(regressor_lm_1)
rstudent = rstudent(regressor_lm_1, infl = influence(regressor_lm_1, do.coef = TRUE))
rstudent_df = as.data.frame(rstudent)
outliers = subset(rstudent_df, rstudent_df$rstudent < (-1.5) | rstudent_df$rstudent > 1.5)
outliers_row = rownames(outliers)
outliers_row = as.numeric(outliers_row)
dataset_outliers = dataset[outliers_row, ]
dataset_nooutliers = dataset[-outliers_row, ]
set.seed(123)
split_1 = sample.split(dataset_nooutliers$Rating, SplitRatio = 0.7)
training_set_1 = subset(dataset_nooutliers, split_1 == TRUE)
test_set_1 = subset(dataset_nooutliers, split_1 == FALSE)
regressor_lm_2 = lm(formula = Rating ~ ., data = training_set_1)
summary(regressor_lm_2)
# Now checking the multicollinearity
library(car)
vif = vif(regressor_lm_2)
which(vif>10)
vif_data = training_set_1[-which(vif > 10)]
regressor_lm_3 = lm(formula = Rating ~ ., data = vif_data)
summary(regressor_lm_3)
predictive = abs(lm.beta(regressor_lm_3))
predictive_df = as.data.frame(predictive)
library("arules", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library("hydroGOF", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library("plotrix", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library("QuantPsyc", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
predictive = abs(lm.beta(regressor_lm_3))
predictive_df = as.data.frame(predictive)
category = ifelse(predictive_df$predictive < 0.02, 'No', ifelse(predictive_df$predictive < 0.1, 'weak', ifelse(predictive_df$predictive < 0.3 , 'Medium', 'High')))
predictive_df$Category = category
predictivedata = vif_data[-c(5,8,9,12,15,19,24,25)]
regressor_lm_Final = lm(formula = Rating ~ ., data = predictivedata)
summary(regressor_lm_Final)
train_fit = fitted(regressor_lm_Final)
test_fit = predict(regressor_lm_Final, test_set_1[-1])
train_model = cbind(train_fit, training_set_1$Rating)
test_model = cbind(test_fit, test_set_1$Rating)
summary(regressor_lm_Final)
library("MLmetrics", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
MAPE(test_fit, test_set_1$Rating)
# Calculating the Standard Error with plotrix package
std.error(residuals(regressor_lm_Final))
RMSE(test_fit, test_set_1$Rating)
# Out of the question, Just checking the performance of other regression models
library(e1071)
regressor_svr = svm(formua = Rating ~ ., data = predictivedata,
type = 'eps-regression', kernel = 'radial')
regressor_svr = svm(formula = Rating ~ ., data = predictivedata,
type = 'eps-regression', kernel = 'radial')
summary(regressor_svr)
regressor_svr = svm(formula = Rating ~ ., data = predictivedata,
type = 'nu-regression', kernel = 'radial')
summary(regressor_svr)
print.summary.table(regressor_svr)
fitted(regressor_svr)
r_square = function(model, actualdata){
r_sq = 1 - (sum((actualdata - fitted(model)) ^ 2) / sum((actualdata - mean(actualdata)) ^ 2))
return(r_sq)
}
r_square(regressor_svr, predictivedata$Rating)
r_square(regressor_lm_Final, predictivedata$Rating)
adj_r_square = function(r_squared, no_pred, sample_size){
adj = 1 - (((1 - r_squared)(sample_size - 1)) / (sample_size - no_pred - 1))
return(adj)
}
length(predictivedata)
nrow(predictivedata)
adj_r_square(r_squared = r_square(regressor_svr, predictivedata$Rating),
no_pred = length(predictivedata), sample_size = nrow(predictivedata))
# Calculating the r-square for the SVR model
r_square(regressor_svr, predictivedata$Rating)
adj_r_square(r_squared = r_square(regressor_svr, predictivedata$Rating),
no_pred = length(predictivedata), sample_size = nrow(predictivedata))
# Calculating the adjusted r-squared for the SVR model
adj_r_square(r_squared = 0.9522495, no_pred = length(predictivedata), sample_size = nrow(predictivedata))
# Calculating the adjusted r-squared for the SVR model
adj_r_square(r_squared = 0.9522495, no_pred = length(predictivedata), sample_size = nrow(predictivedata))
# Calculating the adjusted r-squared for the SVR model
adj_r_square(r_squared = 0.9522495, no_pred = length(predictivedata), sample_size = 11202)
# Calculating the adjusted r-squared for the SVR model
adj_r_square(r_squared = 0.9522495, no_pred = 18, sample_size = 11202)
adj_r_square = function(r_squared, no_pred, sample_size){
adj = 1 - (((1 - r_squared) * (sample_size - 1)) / (sample_size - no_pred - 1))
return(adj)
}
adj_r_square(r_squared = r_square(regressor_svr, predictivedata$Rating),
no_pred = length(predictivedata), sample_size = nrow(predictivedata))
adj_r_square(r_squared = r_square(regressor_lm_Final, predictivedata$Rating),
no_pred = length(predictivedata), sample_size = nrow(predictivedata))
adj_r_square(r_squared = r_square(regressor_svr, predictivedata$Rating),
no_pred = length(predictivedata), sample_size = nrow(predictivedata))
# Calculating MAPE for the SVR model
test_fit_svr = predict(regressor_svr, test_set_1[-1])
MAPE(test_fit_svr, test_set_1$Rating)
# Calculating MAPE with MLmetrics package
MAPE(train_fit, predictivedata$Rating)
MAPE(test_fit, predictivedata[1])
MAPE(test_fit, predictivedata[, 1])
MAPE(test_fit, test_set_1[1])
MAPE(test_fit, test_set_1[, 1])
MAPE(test_fit, test_set_1$Rating)
MAPE(test_fit_svr, test_set_1$Rating)
# Calculating RMSE for the SVR model
RMSE(test_fit_svr, test_set_1$Rating)
residuals(regressor_svr)
# Calculating Standard error for the SVR model
std.error(residuals(regressor_svr))
coefficients(regressor_svr)
