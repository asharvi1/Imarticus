?tapply
table(flags$landmass)
table(flags$animate)
tapply(flags$animate, flags$landmass, mean)
tapply(flags$population, flags$red, summary)
table(flags$population)
tapply(flags$population, flags$landmass, summary)
library(datasets)
data("iris")
?iris
tapply(iris$Sepal.Length, iris$Species, summary)
tapply(iris$Sepal.Length, iris$Species, round(mean, 2))
tapply(iris$Sepal.Length, iris$Species, mean)
head(iris)
apply(iris[, 1:4], 2, mean)
apply(iris[, 1:4], 1, mean)
?apply
apply(iris[, 1:4], mean)
library(mtcars)
library(datasets)
data("mtcars")
mean(mtcars$mpg, mtcars$cyl)
with(mtcars, tapply(mpg, cyl, mean))
tapply(mtcars$mpg, mtcars$cyl, mean)
dim(mtcars)
colnames(mtcars)
apply(mtcars, 2, mean)
split(mtcars, mtcars$cyl)
tapply(mtcars$cyl, mtcars$mpg, mean)
sapply(split(mtcars$mpg, mtcars$cyl), mean)
sapply(mtcars, cyl, mean)
tapply(mtcars$hp, mtcars$cyl, mean)
sample <- tapply(mtcars$hp, mtcars$cyl, mean)
sample
sample(1)
sample(2)
sample
class(sample)
sample[1]
sample[3] - sample[1]
debug(ls)
ls
ls
?ls
swirl()
library(swirl)
swirl()
library(iris)
library(datasets)
data("iris")
?lapply
swirl()
head(flags)
dim(flags)
class(flags)
cls_list <- lapply(flags, class)
cls_list
class(cls_list)
as.character(cls_list)
cls_vect <- sapply(flags, class)
class(cls_vect)
sum(flags$orange)
flag_colors <- flags[,11:17]
head(flag_colors)
lapply(flag_colors, sum)
sapply(flag_colors, sum)
sapply(flag_colors,mean)
flag_shapes <- flags[,19:23]
lapply(flag_shapes, range)
shape_mat <- sapply(flag_shapes, range)
shape_mat
class(shape_mat)
unique(c(3, 4, 5, 5, 5, 6, 6))
unique_vals <- lapply(flags, unique)
unique_vals
lapply(unique_vals, length)
sapply(unique_vals, length)
sapply(flags, unique)
lapply(unique_vals, function(elem) elem[2])
sapply(flags, unique)
vapply(flags, unique, numeric(1))
ok()
sapply(flags, class)
vapply(flags, class, character(1))
?tapply
table(flags$landmass)
table(flag$animate)
table(flags$animate)
tapply(flags$animate, flags$landmass, mean)
tapply(flags$population, flags$red, summary)
tapply(flags$population, flags$landmass, summary)
library(datasets)
data(iris)
head(iris)
tapply(iris$Sepal.Length, iris$Species, mean)
apply(iris, 1, mean)
colMeans(iris)
apply(iris[, 1:4], 1, mean)
rowMeans(iris[, 1:4])
apply(iris, 2, mean)
apply(iris[, 1:4], 2, mean)
library(datasets)
data(mtcars)
sapply(mtcars, cyl, mean)
head(mtcars)
tapply(mtcars$cyl, mtcars$mpg, mean)
tapply(mtcars$mpg, mtcars$cyl, mean)
sapply(split(mtcars$mpg, mtcars$cyl), mean)
apply(mtcars, 2, mean)
with(mtcars, tapply(mpg, cyl, mean))
mean(mtcars$mpg, mtcars$cyl)
apply(mtcars, 2, mean)
split(mtcars, mtcars$cyl)
lapply(mtcars, mean)
sapply(mtcars, cyl, mean)
tapply(mtcars$hp, mtcars$cyl, mean)
209.21429 - 82.63636
ls
debug(ls)
ls
x <- c(rnorm(10), runif(10))
install.packages('RMySQL')
install.packages("arules")
library("arules", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
dataset = longley
linear_model = lm(formula = Employed ~ GNP, data = dataset)
# Visualising the residual values in the graph to identify the outliers
qqnorm(resid(linear_model))
plot(dataset$GNP, dataset$Employed)
plot(dataset$GNP, dataset$Employed, main = 'Simple Linear', xlab = 'GNP', ylab = 'Employed')
abline(lm(Employed ~ GNP))
abline(lm(Employed ~ GNP, data = dataset))
plot(dataset$GNP, dataset$Employed, main = 'Simple Linear', xlab = 'GNP', ylab = 'Employed', col = 'red')
abline(lm(Employed ~ GNP, data = dataset))
plot(dataset$GNP, dataset$Employed, main = 'Simple Linear', xlab = 'GNP', ylab = 'Employed', col = 'red', pch = 21)
abline(lm(Employed ~ GNP, data = dataset))
plot(dataset$GNP, dataset$Employed, main = 'Simple Linear', xlab = 'GNP', ylab = 'Employed', col = 'red', pch = 16)
abline(lm(Employed ~ GNP, data = dataset))
View(dataset)
dataset = dataset[, c(2,7)]
View(dataset)
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
ggplot(data = dataset, aes(x =  dataset$GNP, y = dataset$Employed)) +
geom_point(shape = 1) +
geom_smooth(method = lm)
ggplot(data = dataset, aes(x =  dataset$GNP, y = dataset$Employed)) +
geom_point(shape = 2) +
geom_smooth(method = lm)
ggplot(data = dataset, aes(x =  dataset$GNP, y = dataset$Employed)) +
geom_point(shape = 3) +
geom_smooth(method = lm)
ggplot(data = dataset, aes(x =  dataset$GNP, y = dataset$Employed)) +
geom_point(shape = 4) +
geom_smooth(method = lm)
ggplot(data = dataset, aes(x =  dataset$GNP, y = dataset$Employed)) +
geom_point() +
geom_smooth(method = lm)
load("/Users/ravikiran/Desktop/R/Air Quality UCI/airquality.RData")
a = c(2,1,2,3,5,6,7,8,2)
sum(a[a == 2])
dataset = mtcars
# Creating a regression model
regressor = lm(mpg~cyl+wt+am+gear+carb, data = dataset)
summary(regressor)$coefficients
summary(regressor)$estimate
summary(regressor)$Estimate
summary(regressor)coefficients
summary(regressor)$coefficients
library("QuantPsyc", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
lm.beta(regressor)
summary(regressor)$coefficients
install.packages("sirt")
setwd("~/Desktop/Imarticus/Academics/class practice/R/Case Studies/German Credit Data")
dataset = read.csv('German_Credit_Data.csv')
# Replacing the 2 with 0 in the status column
dataset$Status[dataset$Status == 2] = 0
dataset$Status = as.factor(dataset$Status)
set.seed(123)
split = sample.split(dataset$Status, SplitRatio = 0.7)
library("caTools", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
set.seed(123)
split = sample.split(dataset$Status, SplitRatio = 0.7)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Fitting the logistic regression to the training set
classifier = glm(formula = Status ~ ., family = 'binomial', data = training_set)
# To calculate the pseudo R^2 we must use the nagelkekeR2
classifier_NKR2 = NagelkerkeR2(classifier)
library("fmsb", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
# To calculate the pseudo R^2 we must use the nagelkekeR2 from the fmsb package
classifier_NKR2 = NagelkerkeR2(classifier)
# Identifying the outliers
rstudent = rstudent(classifier, infl = influence(classifier, do.coef = TRUE))
rstudentdf = as.data.frame(rstudent)
outliers = subset(rstudentdf, rstudentdf$rstudent < (-1.5) | rstudentdf$rstudent > 1.5)
outliers_rownames = rownames(outliers)
outliers_rownames = as.numeric(outliers_rownames)
# Creating a outliers dataset from the training set
training_set_o = training_set[outliers_rownames, ]
# Creating a no outliers dataset from the training set
training_set_no = training_set[-outliers_rownames, ]
# Fitting the Logistic Regression model with out the outliers in the training set
classifier_no = glm(formula = Status ~ ., data = training_set_no, family = 'binomial')
# Checking the pseudo R^2 value for the with out outlier data
classifier_no_NKR2 = NagelkerkeR2(classifier_no)
library("car", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
# Checking the multicollinearity between the variables
vif = vif(classifier_no)
vif = as.data.frame(vif)
# Using Boruta package for informating value calculation
boruta = Boruta(Status ~ ., doTrace = 2, data = training_set_no)
library("Boruta", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
# Using Boruta package for informating value calculation
boruta = Boruta(Status ~ ., doTrace = 2, data = training_set_no)
boruta
boruta_decision = boruta$finalDecision
# Removing the unimportant variables
training_set_no_imp = training_set_no[, c(which(boruta_decision == 'Confirmed' | boruta_decision == 'Tentative'), 21)]
# Fitting the model again with the training set with no outliers and with only the important variables checked through boruta
classifier_no_imp = glm(formula = Status ~ ., family = 'binomial', data = training_set_no_imp)
classifier_no_imp_NKR2 = NagelkerkeR2(classifier_no_imp)
# Creating the ROC curve with the ROCR package
train_pred = fitted(classifier_no_imp)
train_pred = as.data.frame(train_pred)
ROCpred = prediction(train_pred$train_pred, training_set_no_imp$Status)
library("ROCR", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
ROCpred = prediction(train_pred$train_pred, training_set_no_imp$Status)
ROCpref = performance(ROCpred, 'tpr', 'fpr')
plot(ROCpref)
# Calculating the AUC value for the classifier_no_imp model from pROC package
auc = auc(training_set_no_imp$Status, train_pred$train_pred)
library("pROC", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
# Calculating the AUC value for the classifier_no_imp model from pROC package
auc = auc(training_set_no_imp$Status, train_pred$train_pred)
# Calculating Loglikelihood
loglik = logLik(classifier_no_imp)
loglik2 = -2 * loglik
loglik2
# Calculating Gini Coefficient from reldist package
gini = gini(as.numeric(training_set_no_imp$Status), train_pred$train_pred)
library("reldist", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
# Calculating Gini Coefficient from reldist package
gini = gini(as.numeric(training_set_no_imp$Status), train_pred$train_pred)
# Calculating Gini Coefficient with ineq package
ineq(training_set_no_imp$Status, type = 'Gini')
library("ineq", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
# Calculating Gini Coefficient with ineq package
ineq(training_set_no_imp$Status, type = 'Gini')
# Calculating Concordant and Discordant pairs from Informationvalue package
concordace = Concordance(training_set_no_imp$Status, train_pred$train_pred)
library("InformationValue", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
# Calculating Concordant and Discordant pairs from Informationvalue package
concordace = Concordance(training_set_no_imp$Status, train_pred$train_pred)
# Calculating KS statastic
ks_tes = ks.test(training_set_no_imp$Status, train_pred$train_pred)
ks_tes
library("Metrics", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
# Calculating RMSE with formula and metrics package
# RMSE = sqrt(mean((training_set_no_imp$Status - train_pred$train_pred) ^ 2))
RMSE = Metrics::rmse(as.numeric(training_set_no_imp$Status), train_pred$train_pred)
# Creating a confusion matrix
# confusion_matrix = table(train_pred$train_pred, training_set_no_imp$Status)# as there are probs in the predicted set, we cannot just use the table
# To tackle this issue we use confusionMatrix from the informationValue package
confusionMatrix(training_set_no_imp$Status, train_pred$train_pred, threshold = 0.5)
# Creating a confusion matrix
# confusion_matrix = table(train_pred$train_pred, training_set_no_imp$Status)# as there are probs in the predicted set, we cannot just use the table
# To tackle this issue we use confusionMatrix from the informationValue package
confusion_matrix = confusionMatrix(training_set_no_imp$Status, train_pred$train_pred, threshold = 0.5)
View(test_set)
test_pred = predict(classifier_no_imp, test_set[ ,-21], type = 'response')
test_pred = as.data.frame(test_pred)
View(train_pred)
## Diagnosing the predicted test set values
ROCpred_test_set = prediction(test_pred$test_pred, test_set$Status)
## Diagnosing the predicted test set values
# Creating a daatframe that contains the test_set Status variable and the predicted test set values
test_set_final = cbind(test_set$Status, test_pred$test_pred)
ROCperf = performance(ROCpred_test_set, 'tpr', 'fpr')
ROCperf_test_set = performance(ROCpred_test_set, 'tpr', 'fpr')
rm(ROCperf)
plot(ROCperf_test_set, main = 'TPR vs FPR for test_set')
plot(ROCperf_test_set, main = 'FPR vs TPR for test_set')
# Calculating the auc values for the test_pred
auc_test_set = auc(test_set$Status, test_pred$test_pred)
# Calculating Gini Coefficient for the test_pred
gini_test_set = Gini(as.numeric(test_set$Status), test_pred$test_pred)
# Calculating Concordat pairs and the discordant pairs for the test_pred
concordance_test_set = Concordance(actuals = test_set$Status, test_pred$test_pred)
# Calculating KS Statastic
kstest_test_set = ks.test(test_set$Status, test_pred$test_pred)
# Creating Confusion Matrix for the test_pred
confusion_matrix_test_set = confusionMatrix(actuals = test_set$Status, predictedScores = test_pred$test_pred, threshold = 0.5)
View(confusion_matrix_test_set)
View(test_set_final)
## Diagnosing the predicted test set values
# Creating a daatframe that contains the test_set Status variable and the predicted test set values
test_set_final = as.data.frame(cbind(test_set$Status, test_pred$test_pred))
View(test_set_final)
View(training_set_no_imp)
View(test_set)
# Creating Confusion Matrix for the test_pred
confusion_matrix_test_set = confusionMatrix(actuals = test_set$Status, predictedScores = test_pred$test_pred, threshold = 0.1)
View(confusion_matrix_test_set)
## Diagnosing the predicted test set values
# Creating a daatframe that contains the test_set Status variable and the predicted test set values
test_set_final = cbind(test_set$Status, test_pred$test_pred)
View(test_set_final)
View(test_set)
## Diagnosing the predicted test set values
# Creating a daatframe that contains the test_set Status variable and the predicted test set values
test_set_final = as.data.frame(cbind(as.numeric(test_set$Status), test_pred$test_pred))
View(test_set_final)
## Diagnosing the predicted test set values
# Creating a daatframe that contains the test_set Status variable and the predicted test set values
test_set_final = as.data.frame(cbind(test_set$Status, test_pred$test_pred))
test_set$Status
as.numeric(test_set$Status)
test_set_final[test_set_final$V1 == 1] = 0
test_set_final[test_set_final$V2 == 2] = 1
test_set_final[test_set_final$V1 == 1] = 0
test_set_final[as.numeric(test_set_final$V1) == 1] = 0
## Diagnosing the predicted test set values
# Creating a daatframe that contains the test_set Status variable and the predicted test set values
test_set_final = as.data.frame(cbind(test_set$Status, test_pred$test_pred))
View(test_set_final)
View(test_set)
test_set$Status = factor(test_set$Status, levels = c(0, 1))
test_set_final = as.data.frame(cbind(test_set$Status, test_pred$test_pred))
test_set_final = as.data.frame(cbind(as.numeric(test_set$Status), test_pred$test_pred))
class(test_set$Status)
as.numeric(test_set$Status)
## Diagnosing the predicted test set values
# Creating a daatframe that contains the test_set Status variable and the predicted test set values
test_set_actuals = as.numeric(test_set$Status)
test_set_actuals
test_set_actuals[test_set_actuals == 1] = 0
test_set_actuals
test_set_actuals[test_set_actuals == 2] = 1
test_set_final = as.data.frame(cbind(test_set_actuals, test_pred$test_pred))
rm(confusion_matrix_test_set)
# Creating Confusion Matrix for the test_pred
confusion_matrix_test_set_0.1 = confusionMatrix(actuals = test_set$Status, predictedScores = test_pred$test_pred, threshold = 0.1)
View(confusion_matrix_test_set_0.1)
confusion_matrix_test_set_0.1$0[1]
confusion_matrix_test_set_0.1[1,1]
sum(confusion_matrix_test_set_0.1)
3+1+87+209
accuracy_cm = function(confusionmatrix){
accuracy = (confusionmatrix[1,1] + confusionmatrix[2,2]) / (sum(confusionmatrix))
}
accuracy_test_pred_0.1 = accuracy_cm(confusion_matrix_test_set_0.1)
accuracy_test_pred_0.1
best_treshold = function(actuals, preds){
b_tresh = numeric(0)
accuracy_v = numeric(0)
for (i in 0.1:0.9){
confusion_matrix = confusionMatrix(actuals = actuals, predictedScores = preds)
accuracy_confusion_matrix = accuracy_cm(confusion_matrix)
b_tresh = c(b_tresh, i)
accuracy_v = c(accuracy_v, accuracy_confusion_matrix)
}
best_treshold_df = as.data.frame(cbind(b_tresh, accuracy_v))
assign('best_treshold_df', best_treshold_df, envir = .GlobalEnv)
}
# Finding the best tresholds
best_treshold_df = best_treshold(actuals = test_set$Status, preds = test_pred$test_pred)
View(best_treshold_df)
best_treshold = function(actuals, preds){
b_tresh = numeric(0)
accuracy_v = numeric(0)
for (i in 0.1:0.9){
confusion_matrix = confusionMatrix(actuals = actuals, predictedScores = preds, threshold = i)
accuracy_confusion_matrix = accuracy_cm(confusion_matrix)
b_tresh = c(b_tresh, i)
accuracy_v = c(accuracy_v, accuracy_confusion_matrix)
}
best_treshold_df = as.data.frame(cbind(b_tresh, accuracy_v))
assign('best_treshold_df', best_treshold_df, envir = .GlobalEnv)
}
# Finding the best tresholds
best_treshold_df = best_treshold(actuals = test_set$Status, preds = test_pred$test_pred)
View(best_treshold_df)
best_treshold = function(actuals, preds){
b_tresh = numeric()
accuracy_v = numeric()
for (i in 0.1:0.9){
confusion_matrix = confusionMatrix(actuals = actuals, predictedScores = preds, threshold = i)
accuracy_confusion_matrix = accuracy_cm(confusion_matrix)
b_tresh = c(b_tresh, i)
accuracy_v = c(accuracy_v, accuracy_confusion_matrix)
}
best_treshold_df = as.data.frame(cbind(b_tresh, accuracy_v))
assign('best_treshold_df', best_treshold_df, envir = .GlobalEnv)
}
# Finding the best tresholds
best_treshold_df = best_treshold(actuals = test_set$Status, preds = test_pred$test_pred)
best_treshold = function(actuals, preds){
b_tresh = numeric()
accuracy_v = numeric()
for (i in 0.1:0.9){
confusion_matrix = confusionMatrix(actuals = actuals, predictedScores = preds, threshold = i)
accuracy_confusion_matrix = accuracy_cm(confusion_matrix)
b_tresh = c(b_tresh, i)
accuracy_v = c(accuracy_v, accuracy_confusion_matrix)
}
best_treshold_df = as.data.frame(cbind(b_tresh, accuracy_v))
assign('best_treshold_df', best_treshold_df, envir = .GlobalEnv)
assign('b_tresh', b_tresh, envir = .GlobalEnv)
assign('accuracy_v', accuracy_v, envir = .GlobalEnv)
}
# Finding the best tresholds
best_treshold_df = best_treshold(actuals = test_set$Status, preds = test_pred$test_pred)
best_treshold = function(actuals, preds){
b_tresh = numeric()
accuracy_v = numeric()
for (i in (0.1):(0.9)){
confusion_matrix = confusionMatrix(actuals = actuals, predictedScores = preds, threshold = i)
accuracy_confusion_matrix = accuracy_cm(confusion_matrix)
b_tresh = c(b_tresh, i)
accuracy_v = c(accuracy_v, accuracy_confusion_matrix)
}
best_treshold_df = as.data.frame(cbind(b_tresh, accuracy_v))
assign('best_treshold_df', best_treshold_df, envir = .GlobalEnv)
assign('b_tresh', b_tresh, envir = .GlobalEnv)
assign('accuracy_v', accuracy_v, envir = .GlobalEnv)
}
# Finding the best tresholds
best_treshold_df = best_treshold(actuals = test_set$Status, preds = test_pred$test_pred)
best_treshold = function(actuals, preds){
b_tresh = numeric()
accuracy_v = numeric()
for (i in 0.1:0.9){
confusion_matrix = confusionMatrix(actuals = actuals, predictedScores = preds, threshold = i)
accuracy_confusion_matrix = accuracy_cm(confusion_matrix)
b_tresh = c(b_tresh, i)
accuracy_v = c(accuracy_v, accuracy_confusion_matrix)
}
best_treshold_df = as.data.frame(cbind(b_tresh, accuracy_v))
assign('best_treshold_df', best_treshold_df, envir = .GlobalEnv)
assign('b_tresh', b_tresh, envir = .GlobalEnv)
assign('accuracy_v', accuracy_v, envir = .GlobalEnv)
}
# Finding the best tresholds
best_treshold_df = best_treshold(actuals = test_set_actuals, preds = test_pred$test_pred)
0.1:0.9
c(0.1:0.9)
seq(0.1, 0.9, by = 0.1)
best_treshold = function(actuals, preds){
b_tresh = numeric()
accuracy_v = numeric()
for (i in seq(0.1, 0.9, by = 0.1)){
confusion_matrix = confusionMatrix(actuals = actuals, predictedScores = preds, threshold = i)
accuracy_confusion_matrix = accuracy_cm(confusion_matrix)
b_tresh = c(b_tresh, i)
accuracy_v = c(accuracy_v, accuracy_confusion_matrix)
}
best_treshold_df = as.data.frame(cbind(b_tresh, accuracy_v))
assign('best_treshold_df', best_treshold_df, envir = .GlobalEnv)
assign('b_tresh', b_tresh, envir = .GlobalEnv)
assign('accuracy_v', accuracy_v, envir = .GlobalEnv)
}
# Finding the best tresholds
best_treshold_df = best_treshold(actuals = test_set_actuals, preds = test_pred$test_pred)
rm(b_tresh)
rm(accuracy_v)
best_treshold = function(actuals, preds){
b_tresh = numeric()
accuracy_v = numeric()
for (i in seq(0.1, 0.9, by = 0.1)){
confusion_matrix = confusionMatrix(actuals = actuals, predictedScores = preds, threshold = i)
accuracy_confusion_matrix = accuracy_cm(confusion_matrix)
b_tresh = c(b_tresh, i)
accuracy_v = c(accuracy_v, accuracy_confusion_matrix)
}
best_treshold_df = as.data.frame(cbind(b_tresh, accuracy_v))
assign('best_treshold_df', best_treshold_df, envir = .GlobalEnv)
}
# Finding the best tresholds
best_treshold_df = best_treshold(actuals = test_set_actuals, preds = test_pred$test_pred)
View(best_treshold_df)
max(best_treshold_df$accuracy_v)
best_treshold_df[max(best_treshold_df$accuracy_v),]
best_treshold_df[which(max(best_treshold_df$accuracy_v))]
best_treshold_df[which(best_treshold_df$accuracy_v) == max(best_treshold_df$accuracy_v)]
best_treshold_df[which(best_treshold_df$accuracy_v) == max(best_treshold_df$accuracy_v),]
best_treshold_df[, which(best_treshold_df$accuracy_v) == max(best_treshold_df$accuracy_v)]
max(best_treshold_df)
best_treshold_df$b_tresh[1]
best_treshold_df$b_tresh[which(best_treshold_df$accuracy_v) == max(best_treshold_df$accuracy_v)]
best_treshold_df$b_tresh[best_treshold_df$accuracy_v]
best_treshold_df$b_tresh[max(best_treshold_df$accuracy_v)]
best_treshold_df[which.max(best_treshold_df$accuracy_v), ]
best_treshold_df[which.max(best_treshold_df$accuracy_v), ]
best_treshold_df$b_tresh[which.max(best_treshold_df$accuracy_v), ]
best_treshold_df$b_tresh[which.max(best_treshold_df$accuracy_v)]
test_set$Status
as.numeric(test_set$Status)
as.integer(test_set$Status)
as.numeric(levels(test_set$Status))
levels.default(test_set$Status)
levels(test_set$Status)[test_set$Status]
as.numeric(levels(test_set$Status)[test_set$Status])
## Diagnosing the predicted test set values
# Creating a daatframe that contains the test_set Status variable and the predicted test set values
test_set_final = as.data.frame(cbind(as.numeric(levels(test_set$Status)[test_set$Status], test_pred$test_pred)))
View(test_set_final)
## Diagnosing the predicted test set values
# Creating a daatframe that contains the test_set Status variable and the predicted test set values
test_set_final = as.data.frame(cbind(as.numeric(levels(test_set$Status)[test_set$Status]), test_pred$test_pred)))
## Diagnosing the predicted test set values
# Creating a daatframe that contains the test_set Status variable and the predicted test set values
test_set_final = as.data.frame(cbind(as.numeric(levels(test_set$Status)[test_set$Status]), test_pred$test_pred))
